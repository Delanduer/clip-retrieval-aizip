{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uT9FwUjk_lRD"
   },
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LIJwsAPIjvnX",
    "outputId": "a8ea45f0-0cf7-4b5c-836a-c7c4f5bbed1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting clip-retrieval\n",
      "  Downloading clip_retrieval-2.35.1-py3-none-any.whl (351 kB)\n",
      "\u001b[K     |████████████████████████████████| 351 kB 2.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting img2dataset\n",
      "  Downloading img2dataset-1.33.0-py3-none-any.whl (34 kB)\n",
      "Collecting fsspec==2022.1.0\n",
      "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 17.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fire<0.5.0,>=0.4.0\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "\u001b[K     |████████████████████████████████| 87 kB 7.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting webdataset<0.2,>=0.1.103\n",
      "  Downloading webdataset-0.1.103-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 5.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: aiohttp<4,>=3.8.1 in /home/junjie/anaconda3/lib/python3.9/site-packages (from clip-retrieval) (3.8.1)\n",
      "Collecting flask<3,>=2.0.3\n",
      "  Downloading Flask-2.2.2-py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 5.8 MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting faiss-cpu<2,>=1.7.2\n",
      "  Downloading faiss_cpu-1.7.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.6 MB 8.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5,>=4.62.3 in /home/junjie/anaconda3/lib/python3.9/site-packages (from clip-retrieval) (4.64.0)\n",
      "Requirement already satisfied: prometheus-client<1,>=0.13.1 in /home/junjie/anaconda3/lib/python3.9/site-packages (from clip-retrieval) (0.13.1)\n",
      "Requirement already satisfied: pandas<2,>=1.1.5 in /home/junjie/anaconda3/lib/python3.9/site-packages (from clip-retrieval) (1.4.2)\n",
      "Collecting pyarrow<8,>=6.0.1\n",
      "  Downloading pyarrow-7.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 26.7 MB 12.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h5py<4,>=3.1.0 in /home/junjie/anaconda3/lib/python3.9/site-packages (from clip-retrieval) (3.6.0)\n",
      "Collecting clip-anytorch<3,>=2.5.0\n",
      "  Downloading clip_anytorch-2.5.0-py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2,>=1.19.5 in /home/junjie/anaconda3/lib/python3.9/site-packages (from clip-retrieval) (1.21.5)\n",
      "Requirement already satisfied: requests<3,>=2.27.1 in /home/junjie/anaconda3/lib/python3.9/site-packages (from clip-retrieval) (2.27.1)\n",
      "Collecting torch<2,>=1.7.1\n",
      "  Downloading torch-1.13.0-cp39-cp39-manylinux1_x86_64.whl (890.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 890.2 MB 15 kB/s  eta 0:00:011     |█████▉                          | 162.3 MB 6.2 MB/s eta 0:01:57     |██████████████████████████████▉ | 857.5 MB 25.8 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting autofaiss<3,>=2.9.6\n",
      "  Downloading autofaiss-2.15.3-py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 8.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting wandb<0.13,>=0.12.10\n",
      "  Downloading wandb-0.12.21-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 20.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting flask-restful<1,>=0.3.9\n",
      "  Downloading Flask_RESTful-0.3.9-py2.py3-none-any.whl (25 kB)\n",
      "Collecting flask-cors<4,>=3.0.10\n",
      "  Downloading Flask_Cors-3.0.10-py2.py3-none-any.whl (14 kB)\n",
      "Collecting torchvision<2,>=0.10.1\n",
      "  Downloading torchvision-0.14.0-cp39-cp39-manylinux1_x86_64.whl (24.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.3 MB 9.1 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting multilingual-clip<2,>=1.0.10\n",
      "  Downloading multilingual_clip-1.0.10-py3-none-any.whl (20 kB)\n",
      "Collecting open-clip-torch<3.0.0,>=2.0.0\n",
      "  Downloading open_clip_torch-2.0.2-py3-none-any.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 13.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentence-transformers<3,>=2.2.0\n",
      "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 5.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting dataclasses<1.0.0,>=0.6\n",
      "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
      "Collecting albumentations<2,>=1.1.0\n",
      "  Downloading albumentations-1.3.0-py3-none-any.whl (123 kB)\n",
      "\u001b[K     |████████████████████████████████| 123 kB 16.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting exifread-nocycle<4,>=3.0.1\n",
      "  Downloading ExifRead_nocycle-3.0.1-py3-none-any.whl (39 kB)\n",
      "Collecting opencv-python<5,>=4.5.5.62\n",
      "  Downloading opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 60.9 MB 233 kB/s eta 0:00:012\n",
      "\u001b[?25hRequirement already satisfied: yarl<2.0,>=1.0 in /home/junjie/anaconda3/lib/python3.9/site-packages (from aiohttp<4,>=3.8.1->clip-retrieval) (1.6.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/junjie/anaconda3/lib/python3.9/site-packages (from aiohttp<4,>=3.8.1->clip-retrieval) (5.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/junjie/anaconda3/lib/python3.9/site-packages (from aiohttp<4,>=3.8.1->clip-retrieval) (2.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/junjie/anaconda3/lib/python3.9/site-packages (from aiohttp<4,>=3.8.1->clip-retrieval) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/junjie/anaconda3/lib/python3.9/site-packages (from aiohttp<4,>=3.8.1->clip-retrieval) (1.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/junjie/anaconda3/lib/python3.9/site-packages (from aiohttp<4,>=3.8.1->clip-retrieval) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/junjie/anaconda3/lib/python3.9/site-packages (from aiohttp<4,>=3.8.1->clip-retrieval) (4.0.1)\n",
      "Collecting qudida>=0.0.4\n",
      "  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
      "Requirement already satisfied: scipy in /home/junjie/anaconda3/lib/python3.9/site-packages (from albumentations<2,>=1.1.0->img2dataset) (1.7.3)\n",
      "Collecting opencv-python-headless>=4.1.1\n",
      "  Downloading opencv_python_headless-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (48.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 48.3 MB 20.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /home/junjie/anaconda3/lib/python3.9/site-packages (from albumentations<2,>=1.1.0->img2dataset) (6.0)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /home/junjie/anaconda3/lib/python3.9/site-packages (from albumentations<2,>=1.1.0->img2dataset) (0.19.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /home/junjie/anaconda3/lib/python3.9/site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp<4,>=3.8.1->clip-retrieval) (4.1.1)\n",
      "Collecting embedding-reader<2,>=1.2.0\n",
      "  Downloading embedding_reader-1.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 2.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex in /home/junjie/anaconda3/lib/python3.9/site-packages (from clip-anytorch<3,>=2.5.0->clip-retrieval) (2022.3.15)\n",
      "Requirement already satisfied: six in /home/junjie/anaconda3/lib/python3.9/site-packages (from fire<0.5.0,>=0.4.0->clip-retrieval) (1.16.0)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-2.1.0-py3-none-any.whl (5.8 kB)\n",
      "Collecting Werkzeug>=2.2.2\n",
      "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
      "\u001b[K     |████████████████████████████████| 232 kB 8.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click>=8.0 in /home/junjie/anaconda3/lib/python3.9/site-packages (from flask<3,>=2.0.3->clip-retrieval) (8.0.4)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /home/junjie/anaconda3/lib/python3.9/site-packages (from flask<3,>=2.0.3->clip-retrieval) (2.0.1)\n",
      "Collecting Jinja2>=3.0\n",
      "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 17.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata>=3.6.0 in /home/junjie/anaconda3/lib/python3.9/site-packages (from flask<3,>=2.0.3->clip-retrieval) (4.11.3)\n",
      "Requirement already satisfied: pytz in /home/junjie/anaconda3/lib/python3.9/site-packages (from flask-restful<1,>=0.3.9->clip-retrieval) (2021.3)\n",
      "Collecting aniso8601>=0.82\n",
      "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 1.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /home/junjie/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=3.6.0->flask<3,>=2.0.3->clip-retrieval) (3.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/junjie/anaconda3/lib/python3.9/site-packages (from Jinja2>=3.0->flask<3,>=2.0.3->clip-retrieval) (2.0.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.5 MB 6.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub\n",
      "  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n",
      "\u001b[K     |████████████████████████████████| 163 kB 10.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /home/junjie/anaconda3/lib/python3.9/site-packages (from pandas<2,>=1.1.5->clip-retrieval) (2.8.2)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /home/junjie/anaconda3/lib/python3.9/site-packages (from qudida>=0.0.4->albumentations<2,>=1.1.0->img2dataset) (1.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/junjie/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.27.1->clip-retrieval) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/junjie/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.27.1->clip-retrieval) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/junjie/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.27.1->clip-retrieval) (3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/junjie/anaconda3/lib/python3.9/site-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset) (21.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/junjie/anaconda3/lib/python3.9/site-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset) (1.3.0)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /home/junjie/anaconda3/lib/python3.9/site-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset) (2.9.0)\n",
      "Requirement already satisfied: networkx>=2.2 in /home/junjie/anaconda3/lib/python3.9/site-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset) (2.7.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/junjie/anaconda3/lib/python3.9/site-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset) (2021.7.2)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /home/junjie/anaconda3/lib/python3.9/site-packages (from scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset) (9.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/junjie/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->scikit-image>=0.16.1->albumentations<2,>=1.1.0->img2dataset) (3.0.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/junjie/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations<2,>=1.1.0->img2dataset) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/junjie/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations<2,>=1.1.0->img2dataset) (2.2.0)\n",
      "Requirement already satisfied: nltk in /home/junjie/anaconda3/lib/python3.9/site-packages (from sentence-transformers<3,>=2.2.0->clip-retrieval) (3.7)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 15.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/junjie/anaconda3/lib/python3.9/site-packages (from huggingface-hub->open-clip-torch<3.0.0,>=2.0.0->clip-retrieval) (3.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 21.0 MB 7.8 MB/s eta 0:00:011    |███████▏                        | 4.7 MB 13.1 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 557.1 MB 13 kB/s  eta 0:00:011  |███▋                            | 35.4 MB 15.9 MB/s eta 0:00:18\n",
      "\u001b[?25hRequirement already satisfied: wheel in /home/junjie/anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2,>=1.7.1->clip-retrieval) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /home/junjie/anaconda3/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch<2,>=1.7.1->clip-retrieval) (61.2.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.6 MB 22.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.10.1-py2.py3-none-any.whl (166 kB)\n",
      "\u001b[K     |████████████████████████████████| 166 kB 10.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting setproctitle\n",
      "  Downloading setproctitle-1.3.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
      "Collecting promise<3,>=2.0\n",
      "  Downloading promise-2.3.tar.gz (19 kB)\n",
      "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /home/junjie/anaconda3/lib/python3.9/site-packages (from wandb<0.13,>=0.12.10->clip-retrieval) (3.19.1)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/junjie/anaconda3/lib/python3.9/site-packages (from wandb<0.13,>=0.12.10->clip-retrieval) (5.8.0)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.29-py3-none-any.whl (182 kB)\n",
      "\u001b[K     |████████████████████████████████| 182 kB 13.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[K     |████████████████████████████████| 140 kB 12.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting braceexpand\n",
      "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/junjie/anaconda3/lib/python3.9/site-packages (from ftfy->clip-anytorch<3,>=2.5.0->clip-retrieval) (0.2.5)\n",
      "Building wheels for collected packages: fire, sentence-transformers, promise, pathtools\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=7209b63f3aeb32899cc7bb82a25f2de55d1bcdc1f07c856f4b7223eae46234bb\n",
      "  Stored in directory: /home/junjie/.cache/pip/wheels/2a/93/86/8cd17bc6c40fb605c3ac549d0b860ef7e84ee5f67bf01a3287\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125940 sha256=24a0bad5df7d1d5fedb311aff37464a7e6fd88793d4d0ecf9c10cc79cf43b334\n",
      "  Stored in directory: /home/junjie/.cache/pip/wheels/71/67/06/162a3760c40d74dd40bc855d527008d26341c2b0ecf3e8e11f\n",
      "  Building wheel for promise (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21503 sha256=de9d6bf56b0242ad333e9280f4ec34f576b584534ded9f232ca7c50522d0b8b9\n",
      "  Stored in directory: /home/junjie/.cache/pip/wheels/e1/e8/83/ddea66100678d139b14bc87692ece57c6a2a937956d2532608\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=b48680b13cd14c41c55a3abbbedf9b45d6580bd5923b56a570d34c4027d821a6\n",
      "  Stored in directory: /home/junjie/.cache/pip/wheels/b7/0a/67/ada2a22079218c75a88361c0782855cc72aebc4d18d0289d05\n",
      "Successfully built fire sentence-transformers promise pathtools\n",
      "Installing collected packages: urllib3, smmap, nvidia-cublas-cu11, opencv-python-headless, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, MarkupSafe, gitdb, Werkzeug, torch, tokenizers, termcolor, shortuuid, setproctitle, sentry-sdk, qudida, pyarrow, promise, pathtools, Jinja2, huggingface-hub, GitPython, fsspec, docker-pycreds, braceexpand, webdataset, wandb, transformers, torchvision, sentencepiece, opencv-python, ftfy, flask, fire, faiss-cpu, exifread-nocycle, embedding-reader, dataclasses, aniso8601, albumentations, sentence-transformers, open-clip-torch, multilingual-clip, img2dataset, flask-restful, flask-cors, clip-anytorch, autofaiss, clip-retrieval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.9\n",
      "    Uninstalling urllib3-1.26.9:\n",
      "      Successfully uninstalled urllib3-1.26.9\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.0.1\n",
      "    Uninstalling MarkupSafe-2.0.1:\n",
      "      Successfully uninstalled MarkupSafe-2.0.1\n",
      "  Attempting uninstall: Werkzeug\n",
      "    Found existing installation: Werkzeug 2.0.3\n",
      "    Uninstalling Werkzeug-2.0.3:\n",
      "      Successfully uninstalled Werkzeug-2.0.3\n",
      "  Attempting uninstall: Jinja2\n",
      "    Found existing installation: Jinja2 2.11.3\n",
      "    Uninstalling Jinja2-2.11.3:\n",
      "      Successfully uninstalled Jinja2-2.11.3\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2022.2.0\n",
      "    Uninstalling fsspec-2022.2.0:\n",
      "      Successfully uninstalled fsspec-2022.2.0\n",
      "  Attempting uninstall: flask\n",
      "    Found existing installation: Flask 1.1.2\n",
      "    Uninstalling Flask-1.1.2:\n",
      "      Successfully uninstalled Flask-1.1.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "anaconda-project 0.10.2 requires ruamel-yaml, which is not installed.\u001b[0m\n",
      "Successfully installed GitPython-3.1.29 Jinja2-3.1.2 MarkupSafe-2.1.1 Werkzeug-2.2.2 albumentations-1.3.0 aniso8601-9.0.1 autofaiss-2.15.3 braceexpand-0.1.7 clip-anytorch-2.5.0 clip-retrieval-2.35.1 dataclasses-0.6 docker-pycreds-0.4.0 embedding-reader-1.5.0 exifread-nocycle-3.0.1 faiss-cpu-1.7.2 fire-0.4.0 flask-2.2.2 flask-cors-3.0.10 flask-restful-0.3.9 fsspec-2022.1.0 ftfy-6.1.1 gitdb-4.0.9 huggingface-hub-0.10.1 img2dataset-1.33.0 multilingual-clip-1.0.10 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 open-clip-torch-2.0.2 opencv-python-4.6.0.66 opencv-python-headless-4.6.0.66 pathtools-0.1.2 promise-2.3 pyarrow-7.0.0 qudida-0.0.4 sentence-transformers-2.2.2 sentencepiece-0.1.97 sentry-sdk-1.10.1 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 termcolor-2.1.0 tokenizers-0.13.1 torch-1.13.0 torchvision-0.14.0 transformers-4.24.0 urllib3-1.26.12 wandb-0.12.21 webdataset-0.1.103\n"
     ]
    }
   ],
   "source": [
    "!pip install clip-retrieval img2dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5-9yk7y_qlW"
   },
   "source": [
    "## Get some image urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SA89YmKtjvnX"
   },
   "outputs": [],
   "source": [
    "#!echo 'https://placekitten.com/200/305' >> myimglist.txt\n",
    "#!echo 'https://placekitten.com/200/304' >> myimglist.txt\n",
    "#!echo 'https://placekitten.com/200/303' >> myimglist.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8Tbn2Kl_t1N"
   },
   "source": [
    "## Download the image urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BVZW6noqjvnY",
    "outputId": "484db3d4-249a-4d61-f2d8-5a0f1817a1b4"
   },
   "outputs": [],
   "source": [
    "#!img2dataset --url_list=myimglist.txt --output_folder=image_folder --thread_count=64 --image_size=256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMW4ncir_1Jt"
   },
   "source": [
    "## Produce embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MvNr8NJRjvnZ",
    "outputId": "1484e573-c84d-4593-c7f6-a461b1d516ca"
   },
   "outputs": [],
   "source": [
    "#!clip-retrieval inference  --input_dataset image_folder --output_folder embedding_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples has been estimated to be 118287\n",
      "Starting the worker\n",
      "dataset is 49\n",
      "Starting work on task 0\n",
      "/home/junjie/anaconda3/lib/python3.9/site-packages/clip/clip.py:160: FutureWarning: 'torch.onnx._patch_torch._node_getitem' is deprecated in version 1.13 and will be removed in version 1.14. Please Internally use '_node_get' in symbolic_helper instead..\n",
      "  if \"value\" in node.attributeNames() and str(node[\"value\"]).startswith(\"cuda\"):\n",
      "warming up with batch size 256 on cuda\n",
      "done warming up in 5.359533309936523s87 \n",
      " sample_per_sec 1138 ; sample_count 231695  "
     ]
    }
   ],
   "source": [
    "!clip-retrieval inference  --input_dataset /ssd/mlrom/Data/coco/cocodataset/images/train2017 --output_folder /home/junjie/clip_test/embedding_coco_train2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aBcl0APqjvnZ",
    "outputId": "14bda9f6-e687-43e7-d7ee-754048cc0c2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'embedding_folder': No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!ls -R embedding_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples has been estimated to be 118287\n",
      "Starting the worker\n",
      "dataset is 49\n",
      "Starting work on task 0\n",
      "warming up with batch size 1024 on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/junjie/anaconda3/lib/python3.9/site-packages/clip/clip.py:160: FutureWarning: 'torch.onnx._patch_torch._node_getitem' is deprecated in version 1.13 and will be removed in version 1.14. Please Internally use '_node_get' in symbolic_helper instead..\n",
      "  if \"value\" in node.attributeNames() and str(node[\"value\"]).startswith(\"cuda\"):\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__/multimodal/model/multimodal_transformer.py\", line 19, in encode_image\n    _0 = self.visual\n    input = torch.to(image, torch.device(\"cuda:0\"), 5, False, False, None)\n    return (_0).forward(input, )\n            ~~~~~~~~~~~ <--- HERE\n  def encode_text(self: __torch__.multimodal.model.multimodal_transformer.Multimodal,\n    input: Tensor) -> Tensor:\n  File \"code/__torch__/multimodal/model/multimodal_transformer.py\", line 95, in forward\n    _39 = self.positional_embedding\n    _40 = self.class_embedding\n    _41 = (self.conv1).forward(input, )\n           ~~~~~~~~~~~~~~~~~~~ <--- HERE\n    _42 = ops.prim.NumToTensor(torch.size(_41, 0))\n    _43 = int(_42)\n  File \"code/__torch__/torch/nn/modules/conv.py\", line 8, in forward\n  def forward(self: __torch__.torch.nn.modules.conv.Conv2d,\n    input: Tensor) -> Tensor:\n    x = torch._convolution(input, self.weight, None, [14, 14], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)\n        ~~~~~~~~~~~~~~~~~~ <--- HERE\n    return x\n  def forward1(self: __torch__.torch.nn.modules.conv.Conv2d,\n\nTraceback of TorchScript, original code (most recent call last):\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py(420): _conv_forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py(423): forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(725): _call_impl\n/root/multimodal-pytorch/multimodal/model/multimodal_transformer.py(85): forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(725): _call_impl\n/root/multimodal-pytorch/multimodal/model/multimodal_transformer.py(221): visual_forward\n/opt/conda/lib/python3.7/site-packages/torch/jit/_trace.py(940): trace_module\nexport_torchscript_models.py(37): export_torchscript_models\n/opt/conda/lib/python3.7/site-packages/fire/core.py(672): _CallAndUpdateTrace\n/opt/conda/lib/python3.7/site-packages/fire/core.py(468): _Fire\n/opt/conda/lib/python3.7/site-packages/fire/core.py(138): Fire\nexport_torchscript_models.py(43): <module>\nRuntimeError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 10.92 GiB total capacity; 1.74 GiB already allocated; 196.38 MiB free; 1.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mclip_retrieval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclip_inference\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m main\n\u001b[0;32m----> 6\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/ssd/mlrom/Data/coco/cocodataset/images/train2017\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/junjie/clip_test/embedding_coco_train2017_pyapi_batchtest_L14_512\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclip_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mViT-L/14\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#\"ViT-B/32\", \"ViT-L/14\"\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_jit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msequential\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/clip_retrieval/clip_inference/main.py:154\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(input_dataset, output_folder, input_format, cache_path, batch_size, num_prepro_workers, enable_text, enable_image, enable_metadata, write_batch_size, wds_image_key, wds_caption_key, clip_model, mclip_model, use_mclip, use_jit, distribution_strategy, wds_number_file_per_input_file, output_partition_count, wandb_project, enable_wandb, clip_cache_path, slurm_job_name, slurm_partition, slurm_nodes, slurm_job_comment, slurm_nodelist, slurm_exclude, slurm_job_timeout, slurm_cache_path, slurm_verbose_wait)\u001b[0m\n\u001b[1;32m    146\u001b[0m logger_reader \u001b[38;5;241m=\u001b[39m LoggerReader(\n\u001b[1;32m    147\u001b[0m     stats_folder\u001b[38;5;241m=\u001b[39moutput_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/stats\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    148\u001b[0m     wandb_project\u001b[38;5;241m=\u001b[39mwandb_project,\n\u001b[1;32m    149\u001b[0m     enable_wandb\u001b[38;5;241m=\u001b[39menable_wandb,\n\u001b[1;32m    150\u001b[0m )\n\u001b[1;32m    152\u001b[0m logger_reader\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m--> 154\u001b[0m \u001b[43mdistributor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m logger_reader\u001b[38;5;241m.\u001b[39mend()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/clip_retrieval/clip_inference/distributor.py:17\u001b[0m, in \u001b[0;36mSequentialDistributor.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    call a single `worker(...)` and pass it everything.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mworker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworker_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/clip_retrieval/clip_inference/worker.py:122\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(tasks, input_dataset, output_folder, output_partition_count, input_format, cache_path, batch_size, num_prepro_workers, enable_text, enable_image, enable_metadata, wds_image_key, wds_caption_key, clip_model, mclip_model, use_mclip, use_jit, clip_cache_path)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m tasks:\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting work on task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 122\u001b[0m     \u001b[43mrunner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/clip_retrieval/clip_inference/runner.py:29\u001b[0m, in \u001b[0;36mRunner.__call__\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, i):\n\u001b[1;32m     28\u001b[0m     sampler \u001b[38;5;241m=\u001b[39m Sampler(i, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_partition_count)\n\u001b[0;32m---> 29\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreader_builder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msampler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     writer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter_builder(i)\n\u001b[1;32m     31\u001b[0m     mapper \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmapper_builder()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/clip_retrieval/clip_inference/worker.py:52\u001b[0m, in \u001b[0;36mworker.<locals>.reader_builder\u001b[0;34m(sampler)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreader_builder\u001b[39m(sampler):\n\u001b[0;32m---> 52\u001b[0m     _, preprocess \u001b[38;5;241m=\u001b[39m \u001b[43mload_clip\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclip_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_jit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_jit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_cache_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclip_cache_path\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_format \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m FilesReader(\n\u001b[1;32m     57\u001b[0m             sampler,\n\u001b[1;32m     58\u001b[0m             preprocess,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m             enable_metadata\u001b[38;5;241m=\u001b[39menable_metadata,\n\u001b[1;32m     65\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/clip_retrieval/load_clip.py:77\u001b[0m, in \u001b[0;36mload_clip\u001b[0;34m(clip_model, use_jit, warmup_batch_size, clip_cache_path, device)\u001b[0m\n\u001b[1;32m     75\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwarming up with batch size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwarmup_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 77\u001b[0m \u001b[43mwarmup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwarmup_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m duration \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone warming up in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/clip_retrieval/load_clip.py:90\u001b[0m, in \u001b[0;36mwarmup\u001b[0;34m(batch_size, device, preprocess, model)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 90\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m         model\u001b[38;5;241m.\u001b[39mencode_text(text_tokens)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript, serialized code (most recent call last):\n  File \"code/__torch__/multimodal/model/multimodal_transformer.py\", line 19, in encode_image\n    _0 = self.visual\n    input = torch.to(image, torch.device(\"cuda:0\"), 5, False, False, None)\n    return (_0).forward(input, )\n            ~~~~~~~~~~~ <--- HERE\n  def encode_text(self: __torch__.multimodal.model.multimodal_transformer.Multimodal,\n    input: Tensor) -> Tensor:\n  File \"code/__torch__/multimodal/model/multimodal_transformer.py\", line 95, in forward\n    _39 = self.positional_embedding\n    _40 = self.class_embedding\n    _41 = (self.conv1).forward(input, )\n           ~~~~~~~~~~~~~~~~~~~ <--- HERE\n    _42 = ops.prim.NumToTensor(torch.size(_41, 0))\n    _43 = int(_42)\n  File \"code/__torch__/torch/nn/modules/conv.py\", line 8, in forward\n  def forward(self: __torch__.torch.nn.modules.conv.Conv2d,\n    input: Tensor) -> Tensor:\n    x = torch._convolution(input, self.weight, None, [14, 14], [0, 0], [1, 1], False, [0, 0], 1, False, False, True, True)\n        ~~~~~~~~~~~~~~~~~~ <--- HERE\n    return x\n  def forward1(self: __torch__.torch.nn.modules.conv.Conv2d,\n\nTraceback of TorchScript, original code (most recent call last):\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py(420): _conv_forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py(423): forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(725): _call_impl\n/root/multimodal-pytorch/multimodal/model/multimodal_transformer.py(85): forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(709): _slow_forward\n/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py(725): _call_impl\n/root/multimodal-pytorch/multimodal/model/multimodal_transformer.py(221): visual_forward\n/opt/conda/lib/python3.7/site-packages/torch/jit/_trace.py(940): trace_module\nexport_torchscript_models.py(37): export_torchscript_models\n/opt/conda/lib/python3.7/site-packages/fire/core.py(672): _CallAndUpdateTrace\n/opt/conda/lib/python3.7/site-packages/fire/core.py(468): _Fire\n/opt/conda/lib/python3.7/site-packages/fire/core.py(138): Fire\nexport_torchscript_models.py(43): <module>\nRuntimeError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 10.92 GiB total capacity; 1.74 GiB already allocated; 196.38 MiB free; 1.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n"
     ]
    }
   ],
   "source": [
    "# python version of use clip_retrieval inference\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\n",
    "from clip_retrieval.clip_inference.main import main\n",
    "\n",
    "main(\n",
    "    input_dataset=\"/ssd/mlrom/Data/coco/cocodataset/images/train2017\", \n",
    "    output_folder=\"/home/junjie/clip_test/embedding_coco_train2017_pyapi_batchtest_L14_512\", \n",
    "    batch_size=512, \n",
    "    clip_model=\"ViT-L/14\",#\"ViT-B/32\", \"ViT-L/14\"\n",
    "    use_jit=True,\n",
    "    distribution_strategy=\"sequential\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'start_time': 772576309346.6606, 'end_time': 772576312744.5845, 'read_duration': 5.8528458923101425, 'inference_duration': 3392.022862239741, 'write_duration': 0.04564789496362209, 'total_duration': 3397.924411058426, 'sample_count': 118287}\n"
     ]
    }
   ],
   "source": [
    "# read stats from inference\n",
    "import os, glob, json\n",
    "#output_folder= \"/home/junjie/clip_test/embedding_coco_train2017\"\n",
    "#output_folder= \"/home/junjie/clip_test/embedding_coco_train2017_pyapi\"\n",
    "output_folder= \"/home/junjie/clip_test/embedding_coco_train2017_pyapi_Vitl14_nojit\"\n",
    "#output_folder= \"/home/junjie/clip_test/embedding_coco_train2017_pyapi_Vitl14_pyspark\"\n",
    "jfile_path = os.path.join(output_folder, \"stats\")\n",
    "jfile = glob.glob(jfile_path+\"/*.json\")\n",
    "#print(len(jfile))\n",
    "\n",
    "with open(jfile[0]) as stats_file:\n",
    "    stats = json.load(stats_file)\n",
    "    print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Am62ARgs_3_e"
   },
   "source": [
    "## Produce knn indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xwzha2vY6OnP",
    "outputId": "63e1d45a-1c13-4f84-8a45-1c7196fb7eb6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-17 10:38:54,682 [INFO]: Using 48 omp threads (processes), consider increasing --nb_cores if you have more\n",
      "2022-11-17 10:38:54,682 [INFO]: Launching the whole pipeline 11/17/2022, 10:38:54\n",
      "2022-11-17 10:38:54,683 [INFO]: Reading total number of vectors and dimension 11/17/2022, 10:38:54\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 650.38it/s]\n",
      "2022-11-17 10:38:54,727 [INFO]: There are 118287 embeddings of dim 768\n",
      "2022-11-17 10:38:54,727 [INFO]: >>> Finished \"Reading total number of vectors and dimension\" in 0.0446 secs\n",
      "2022-11-17 10:38:54,727 [INFO]: \tCompute estimated construction time of the index 11/17/2022, 10:38:54\n",
      "2022-11-17 10:38:54,728 [INFO]: \t\t-> Train: 16.7 minutes\n",
      "2022-11-17 10:38:54,728 [INFO]: \t\t-> Add: 1.0 seconds\n",
      "2022-11-17 10:38:54,728 [INFO]: \t\tTotal: 16.7 minutes\n",
      "2022-11-17 10:38:54,728 [INFO]: \t>>> Finished \"Compute estimated construction time of the index\" in 0.0005 secs\n",
      "2022-11-17 10:38:54,728 [INFO]: \tChecking that your have enough memory available to create the index 11/17/2022, 10:38:54\n",
      "2022-11-17 10:38:54,729 [INFO]: 413.0MB of memory will be needed to build the index (more might be used if you have more)\n",
      "2022-11-17 10:38:54,729 [INFO]: \t>>> Finished \"Checking that your have enough memory available to create the index\" in 0.0009 secs\n",
      "2022-11-17 10:38:54,729 [INFO]: \tSelecting most promising index types given data characteristics 11/17/2022, 10:38:54\n",
      "2022-11-17 10:38:54,729 [INFO]: \t>>> Finished \"Selecting most promising index types given data characteristics\" in 0.0001 secs\n",
      "2022-11-17 10:38:54,729 [INFO]: \tCreating the index 11/17/2022, 10:38:54\n",
      "2022-11-17 10:38:54,729 [INFO]: \t\t-> Instanciate the index HNSW32 11/17/2022, 10:38:54\n",
      "2022-11-17 10:38:54,730 [INFO]: \t\t>>> Finished \"-> Instanciate the index HNSW32\" in 0.0002 secs\n",
      "2022-11-17 10:38:54,730 [INFO]: \t\t-> Adding the vectors to the index 11/17/2022, 10:38:54\n",
      "2022-11-17 10:38:54,730 [INFO]: The memory available for adding the vectors is 15.6GB(total available - used by the index)\n",
      "2022-11-17 10:38:54,730 [INFO]: Using a batch size of 325520 (memory overhead 953.7MB)\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:04<00:00,  1.19s/it]\n",
      "2022-11-17 10:38:59,485 [INFO]: \tComputing best hyperparameters for index /home/junjie/clip_test/index_coco_train2017_pyapi_Vitl14_pyspark/image.index 11/17/2022, 10:38:59\n",
      "2022-11-17 10:39:42,179 [INFO]: \t>>> Finished \"Computing best hyperparameters for index /home/junjie/clip_test/index_coco_train2017_pyapi_Vitl14_pyspark/image.index\" in 42.6934 secs\n",
      "2022-11-17 10:39:42,180 [INFO]: The best hyperparameters are: efSearch=731\n",
      "2022-11-17 10:39:42,180 [INFO]: \tCompute fast metrics 11/17/2022, 10:39:42\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]\n",
      "2022-11-17 10:39:53,610 [INFO]: \t>>> Finished \"Compute fast metrics\" in 11.4302 secs\n",
      "2022-11-17 10:39:53,611 [INFO]: \tSaving the index on local disk 11/17/2022, 10:39:53\n",
      "2022-11-17 10:39:54,012 [INFO]: \t>>> Finished \"Saving the index on local disk\" in 0.4009 secs\n",
      "2022-11-17 10:39:54,014 [INFO]: \t\t>>> Finished \"-> Adding the vectors to the index\" in 59.2835 secs\n",
      "2022-11-17 10:39:54,014 [INFO]: {\n",
      "2022-11-17 10:39:54,014 [INFO]: \tindex_key: HNSW32\n",
      "2022-11-17 10:39:54,014 [INFO]: \tindex_param: efSearch=731\n",
      "2022-11-17 10:39:54,014 [INFO]: \tindex_path: /home/junjie/clip_test/index_coco_train2017_pyapi_Vitl14_pyspark/image.index\n",
      "2022-11-17 10:39:54,014 [INFO]: \tsize in bytes: 395578902\n",
      "2022-11-17 10:39:54,014 [INFO]: \tavg_search_speed_ms: 10.760160425417526\n",
      "2022-11-17 10:39:54,014 [INFO]: \t99p_search_speed_ms: 32.658658446744106\n",
      "2022-11-17 10:39:54,014 [INFO]: \treconstruction error %: 0.0\n",
      "2022-11-17 10:39:54,014 [INFO]: \tnb vectors: 118287\n",
      "2022-11-17 10:39:54,014 [INFO]: \tvectors dimension: 768\n",
      "2022-11-17 10:39:54,014 [INFO]: \tcompression ratio: 0.9185971803925984\n",
      "2022-11-17 10:39:54,014 [INFO]: }\n",
      "2022-11-17 10:39:54,014 [INFO]: \t>>> Finished \"Creating the index\" in 59.2848 secs\n",
      "2022-11-17 10:39:54,014 [INFO]: >>> Finished \"Launching the whole pipeline\" in 59.3317 secs\n"
     ]
    }
   ],
   "source": [
    "# index Vit-L/14 Sequential\n",
    "!clip-retrieval index --embeddings_folder=/home/junjie/clip_test/embedding_coco_train2017_pyapi_Vitl14_pyspark --index_folder=/home/junjie/clip_test/index_coco_train2017_pyapi_Vitl14_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-17 10:34:26,187 [INFO]: Using 48 omp threads (processes), consider increasing --nb_cores if you have more\n",
      "2022-11-17 10:34:26,187 [INFO]: Launching the whole pipeline 11/17/2022, 10:34:26\n",
      "2022-11-17 10:34:26,187 [INFO]: Reading total number of vectors and dimension 11/17/2022, 10:34:26\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 740.78it/s]\n",
      "2022-11-17 10:34:26,230 [INFO]: There are 118287 embeddings of dim 512\n",
      "2022-11-17 10:34:26,230 [INFO]: >>> Finished \"Reading total number of vectors and dimension\" in 0.0431 secs\n",
      "2022-11-17 10:34:26,230 [INFO]: \tCompute estimated construction time of the index 11/17/2022, 10:34:26\n",
      "2022-11-17 10:34:26,230 [INFO]: \t\t-> Train: 16.7 minutes\n",
      "2022-11-17 10:34:26,230 [INFO]: \t\t-> Add: 0.7 seconds\n",
      "2022-11-17 10:34:26,230 [INFO]: \t\tTotal: 16.7 minutes\n",
      "2022-11-17 10:34:26,231 [INFO]: \t>>> Finished \"Compute estimated construction time of the index\" in 0.0003 secs\n",
      "2022-11-17 10:34:26,231 [INFO]: \tChecking that your have enough memory available to create the index 11/17/2022, 10:34:26\n",
      "2022-11-17 10:34:26,231 [INFO]: 285.9MB of memory will be needed to build the index (more might be used if you have more)\n",
      "2022-11-17 10:34:26,231 [INFO]: \t>>> Finished \"Checking that your have enough memory available to create the index\" in 0.0008 secs\n",
      "2022-11-17 10:34:26,232 [INFO]: \tSelecting most promising index types given data characteristics 11/17/2022, 10:34:26\n",
      "2022-11-17 10:34:26,232 [INFO]: \t>>> Finished \"Selecting most promising index types given data characteristics\" in 0.0000 secs\n",
      "2022-11-17 10:34:26,232 [INFO]: \tCreating the index 11/17/2022, 10:34:26\n",
      "2022-11-17 10:34:26,232 [INFO]: \t\t-> Instanciate the index HNSW32 11/17/2022, 10:34:26\n",
      "2022-11-17 10:34:26,232 [INFO]: \t\t>>> Finished \"-> Instanciate the index HNSW32\" in 0.0003 secs\n",
      "2022-11-17 10:34:26,232 [INFO]: \t\t-> Adding the vectors to the index 11/17/2022, 10:34:26\n",
      "2022-11-17 10:34:26,232 [INFO]: The memory available for adding the vectors is 15.7GB(total available - used by the index)\n",
      "2022-11-17 10:34:26,232 [INFO]: Using a batch size of 488281 (memory overhead 953.7MB)\n",
      "100%|█████████████████████████████████████████████| 3/3 [00:03<00:00,  1.25s/it]\n",
      "2022-11-17 10:34:29,984 [INFO]: \tComputing best hyperparameters for index /home/junjie/clip_test/index_coco_train2017_pyapi_pyspark/image.index 11/17/2022, 10:34:29\n",
      "2022-11-17 10:35:00,924 [INFO]: \t>>> Finished \"Computing best hyperparameters for index /home/junjie/clip_test/index_coco_train2017_pyapi_pyspark/image.index\" in 30.9387 secs\n",
      "2022-11-17 10:35:00,924 [INFO]: The best hyperparameters are: efSearch=915\n",
      "2022-11-17 10:35:00,925 [INFO]: \tCompute fast metrics 11/17/2022, 10:35:00\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]\n",
      "2022-11-17 10:35:11,750 [INFO]: \t>>> Finished \"Compute fast metrics\" in 10.8254 secs\n",
      "2022-11-17 10:35:11,751 [INFO]: \tSaving the index on local disk 11/17/2022, 10:35:11\n",
      "2022-11-17 10:35:12,029 [INFO]: \t>>> Finished \"Saving the index on local disk\" in 0.2786 secs\n",
      "2022-11-17 10:35:12,031 [INFO]: \t\t>>> Finished \"-> Adding the vectors to the index\" in 45.7983 secs\n",
      "2022-11-17 10:35:12,031 [INFO]: {\n",
      "2022-11-17 10:35:12,031 [INFO]: \tindex_key: HNSW32\n",
      "2022-11-17 10:35:12,031 [INFO]: \tindex_param: efSearch=915\n",
      "2022-11-17 10:35:12,031 [INFO]: \tindex_path: /home/junjie/clip_test/index_coco_train2017_pyapi_pyspark/image.index\n",
      "2022-11-17 10:35:12,031 [INFO]: \tsize in bytes: 274453014\n",
      "2022-11-17 10:35:12,031 [INFO]: \tavg_search_speed_ms: 9.525284638539667\n",
      "2022-11-17 10:35:12,031 [INFO]: \t99p_search_speed_ms: 23.022338580340143\n",
      "2022-11-17 10:35:12,031 [INFO]: \treconstruction error %: 0.0\n",
      "2022-11-17 10:35:12,031 [INFO]: \tnb vectors: 118287\n",
      "2022-11-17 10:35:12,031 [INFO]: \tvectors dimension: 512\n",
      "2022-11-17 10:35:12,031 [INFO]: \tcompression ratio: 0.8826712174492644\n",
      "2022-11-17 10:35:12,031 [INFO]: }\n",
      "2022-11-17 10:35:12,031 [INFO]: \t>>> Finished \"Creating the index\" in 45.7994 secs\n",
      "2022-11-17 10:35:12,031 [INFO]: >>> Finished \"Launching the whole pipeline\" in 45.8443 secs\n"
     ]
    }
   ],
   "source": [
    "# index no jit\n",
    "!clip-retrieval index --embeddings_folder=/home/junjie/clip_test/embedding_coco_train2017_pyapi_pyspark --index_folder=/home/junjie/clip_test/index_coco_train2017_pyapi_pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|model|JIT|distribution|read Duration|inference_duration|write_duration|total_duration|sample_count|size|\n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|ViT-B/32|yes|sequential|37.42|164.62|0.02|202.06(3.3 min)|118287|121126016 (116 Mb)|\n",
    "|ViT-B/32|no|sequential|15.87|186.23|0.02|202.13|118287|121126016|\n",
    "|ViT-B/32|yes|pyspark|32.67|166.09|0.03|198.80|118287|121126016|\n",
    "|ViT-L/64|yes|sequential|6.10|3220.61|0.05|3226.76(53.8 min)|118287|181688960 (174 Mb)|\n",
    "|ViT-L/64|no|sequential|5.85|3392.02|0.05|3397.92|118287|181688960|\n",
    "|ViT-L/64|yes|pyspark|6.07|3220.89|0.05|3227.01|118287|181688960|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gL_X73OY_6TW"
   },
   "source": [
    "## Use the index to get a subset of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "COVo6tHQjvnZ",
    "outputId": "57c02131-5f3a-417b-fd53-36ef5fef1061"
   },
   "outputs": [],
   "source": [
    "#!clip-retrieval filter --query \"cat\" --output_folder \"cat/\" --indice_folder \"index_folder\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wmVuLCKmubsI",
    "outputId": "64547d42-80cc-45a0-d8c6-320f007a77c8"
   },
   "outputs": [],
   "source": [
    "#!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KOdR2ybtjvna",
    "outputId": "219d38d5-f4b1-46b6-b178-c5da880431d0"
   },
   "outputs": [],
   "source": [
    "#ls -R cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 273
    },
    "id": "GHtA2Jlajvna",
    "outputId": "3448f92e-d3de-48ae-8ba6-807d414d45fb"
   },
   "outputs": [],
   "source": [
    "#from IPython.display import Image\n",
    "#Image(filename='cat/000000000.jpg') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcvl9hog_-Lg"
   },
   "source": [
    "## Run the knn service backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8mKtpVPi6jiZ",
    "outputId": "a7382021-c907-44ba-ecf1-4fd864c63089"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo '{\"coco_train2017\": \"/home/junjie/clip_test/index_coco_train2017_pyapi\", \"coco_train2017_nojit\": \"/home/junjie/clip_test/index_coco_train2017_pyapi_nojit\", \"coco_train2017_pyspark\": \"/home/junjie/clip_test/index_coco_train2017_pyapi_pyspark\"}' > /home/junjie/clip_test/indices_paths_B_32.json\n",
    "echo '{\"coco_train2017_vitl14\": \"/home/junjie/clip_test/index_coco_train2017_pyapi_Vitl14\", \"coco_train2017_vitl14_nojit\": \"/home/junjie/clip_test/index_coco_train2017_pyapi_Vitl14_nojit\", \"coco_train2017_vitl14_pyspark\": \"/home/junjie/clip_test/index_coco_train2017_pyapi_Vitl14_pyspark\"}' > /home/junjie/clip_test/indices_paths_L_14.json\n",
    "#npm install -g localtunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUCDh4cq7RgW"
   },
   "outputs": [],
   "source": [
    "# after running the next cell, visit the localtunnel url once then go to\n",
    "# https://rom1504.github.io/clip-retrieval/?back=<your local tunnel url here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q6SaDruy6SOJ",
    "outputId": "a544d2c1-d3d0-4267-a12b-c41c3d599e1e"
   },
   "outputs": [],
   "source": [
    "#from threading import Thread\n",
    "#\n",
    "#def app():\n",
    "#  !clip-retrieval back --port 1234 --indices-paths indices_paths.json\n",
    "#\n",
    "#if __name__ == '__main__':\n",
    "#    t1 = Thread(target = app)\n",
    "#    a = t1.start()\n",
    "#    !lt --port 1234"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "clip-retrieval-getting-started.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "843de08df30066b821f0437d83317f7e657c9d58c210bb967a72474dd7dcb832"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
